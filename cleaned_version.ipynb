{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor, VotingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"/Users/elenalickel/Desktop/Machine Learning /Airbnb_NYC_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data.fillna(0, inplace=True)  # Adjust as necessary for your analysis\n",
    "# Convert the data to string, handling non-string and missing values\n",
    "data['name'] = data['name'].fillna('')  # Replace NaN with empty string\n",
    "data['name'] = data['name'].apply(lambda x: str(x) if not isinstance(x, str) else x)\n",
    "# Convert the data to string, handling non-string and missing values\n",
    "data['host_name'] = data['host_name'].fillna('')  # Replace NaN with empty string\n",
    "data['host_name'] = data['host_name'].apply(lambda x: str(x) if not isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis\n",
    "print(data.columns)\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Convert categorical variables using one-hot encoding\n",
    "categorical_data = pd.get_dummies(data[['neighbourhood_group', 'room_type']], drop_first=True)\n",
    "numeric_data_with_categorical = pd.concat([numeric_data, categorical_data], axis=1)\n",
    "\n",
    "# Updated correlation matrix with categorical data\n",
    "correlation_matrix = numeric_data_with_categorical.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text mining on 'name' and 'host_name' using TfidfVectorizer\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=' ')\n",
    "\n",
    "# Fit and transform, then flatten the array to 1D for 'name'\n",
    "data['name'] = imputer.fit_transform(data[['name']]).ravel()\n",
    "# Fit and transform, then flatten the array to 1D for 'host_name'\n",
    "data['host_name'] = imputer.fit_transform(data[['host_name']]).ravel()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "name_features = tfidf_vectorizer.fit_transform(data['name']).toarray()\n",
    "host_name_features = tfidf_vectorizer.fit_transform(data['host_name']).toarray()\n",
    "\n",
    "# Combine these features with existing numeric features\n",
    "combined_features = np.hstack((numeric_data_with_categorical.values, name_features, host_name_features))\n",
    "\n",
    "# Fit LDA to the name features only\n",
    "lda = LatentDirichletAllocation(n_components=10)\n",
    "lda_features = lda.fit_transform(name_features)\n",
    "print(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated outlier detection\n",
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "outliers = iso_forest.fit_predict(data[['price']])\n",
    "data = data[outliers != -1]\n",
    "\n",
    "# Impute missing values across all numeric features\n",
    "data_no_price = data.drop('price', axis=1)  # Exclude the 'price' column\n",
    "imputer = KNNImputer(n_neighbors=5)  # Impute missing values based on 5 nearest neighbors\n",
    "numeric_data_imputed = imputer.fit_transform(data_no_price.select_dtypes(include=['float64', 'int64']))  # Apply imputation to numeric features\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=5)  # Reduce the dimensionality to 5 principal components\n",
    "principal_components = pca.fit_transform(numeric_data_imputed)  # Apply PCA transformation to the imputed numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training xgboost, randomforest, supportvector\n",
    "X_train, X_test, y_train, y_test = train_test_split(principal_components, data['price'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Individual model setup\n",
    "xgb = XGBRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "svm = make_pipeline(StandardScaler(), SVR())\n",
    "\n",
    "# Train each model separately\n",
    "xgb.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for each model\n",
    "predictions_xgb = xgb.predict(X_test)\n",
    "predictions_rf = rf.predict(X_test)\n",
    "predictions_svm = svm.predict(X_test)\n",
    "\n",
    "# Calculate and print MAE for each model\n",
    "mae_xgb = mean_absolute_error(y_test, predictions_xgb)\n",
    "mae_rf = mean_absolute_error(y_test, predictions_rf)\n",
    "mae_svm = mean_absolute_error(y_test, predictions_svm)\n",
    "\n",
    "print(f'XGBRegressor Mean Absolute Error: {mae_xgb}')\n",
    "print(f'RandomForestRegressor Mean Absolute Error: {mae_rf}')\n",
    "print(f'Support Vector Machine Mean Absolute Error: {mae_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def build_nn_model(n_units=128, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(n_units, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(int(n_units / 2), activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "nn_model = KerasRegressor(model=build_nn_model, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting with NN\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Setup the voting regressor with the NN model included\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb),\n",
    "        ('rf', rf),\n",
    "        ('svm', svm),\n",
    "        ('nn', nn_model)  # Include the NN model\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the voting regressor\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Combined Mean Absolute Error: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training - Voting without NN\n",
    "X_train, X_test, y_train, y_test = train_test_split(principal_components, data['price'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensemble models and voting regressor setup\n",
    "xgb = XGBRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "svm = make_pipeline(StandardScaler(), SVR())\n",
    "voting_regressor = VotingRegressor(estimators=[('xgb', xgb), ('rf', rf), ('svm', svm)])\n",
    "\n",
    "# Train and predict\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search spaces for Bayesian optimization\n",
    "search_spaces = {\n",
    "    'rf__n_estimators': (10, 200),\n",
    "    'rf__max_depth': (5, 50),\n",
    "    'xgb__n_estimators': (10, 200),\n",
    "    'xgb__learning_rate': (0.01, 0.5),\n",
    "    'svm__svr__C': (0.1, 1000),\n",
    "    'svm__svr__gamma': (0.001, 1.0),\n",
    "    'nn__n_units': (50, 200), \n",
    "    'nn__dropout_rate': (0.1, 0.5) \n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Add the neural network to the voting regressor\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb),\n",
    "        ('rf', rf),\n",
    "        ('svm', svm),\n",
    "        ('nn', nn_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Bayesian optimization\n",
    "bayes_search = BayesSearchCV(estimator=voting_regressor, search_spaces=search_spaces, n_iter=32, cv=3)\n",
    "bayes_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "# Load the validation dataset\n",
    "validation_data = pd.read_csv('/Users/elenalickel/Desktop/Machine Learning /Airbnb_NYC_2019_eval_no_price.csv')\n",
    "\n",
    "# Handle missing values for non-text features\n",
    "validation_data.fillna(0, inplace=True)\n",
    "\n",
    "# Handle missing values for text features and ensure all data is of type string\n",
    "validation_data['name'] = validation_data['name'].fillna('').apply(lambda x: str(x))\n",
    "validation_data['host_name'] = validation_data['host_name'].fillna('').apply(lambda x: str(x))\n",
    "\n",
    "# Initialize and fit SimpleImputer on the training data\n",
    "name_imputer = SimpleImputer(strategy='constant', fill_value='Missing Name')\n",
    "host_name_imputer = SimpleImputer(strategy='constant', fill_value='Missing Host')\n",
    "name_imputer.fit(data[['name']])\n",
    "host_name_imputer.fit(data[['host_name']])\n",
    "\n",
    "# Transform validation data using the already fitted imputers\n",
    "validation_data['name'] = name_imputer.transform(validation_data[['name']]).ravel()\n",
    "validation_data['host_name'] = host_name_imputer.transform(validation_data[['host_name']]).ravel()\n",
    "\n",
    "# Initialize TfidfVectorizer and fit on the training data\n",
    "name_tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "host_name_tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "name_tfidf_vectorizer.fit(data['name'].fillna('').apply(str))\n",
    "host_name_tfidf_vectorizer.fit(data['host_name'].fillna('').apply(str))\n",
    "\n",
    "# Transform text features in the validation dataset using the fitted vectorizers\n",
    "name_features_val = name_tfidf_vectorizer.transform(validation_data['name']).toarray()\n",
    "host_name_features_val = host_name_tfidf_vectorizer.transform(validation_data['host_name']).toarray()\n",
    "\n",
    "# Select only the numeric features that were also present during the fitting of the imputer\n",
    "numeric_features_train = data.select_dtypes(include=[np.number]).drop(columns=['price'], errors='ignore')\n",
    "numeric_features_val = validation_data[numeric_features_train.columns]\n",
    "\n",
    "# For the training data, exclude 'price' if it's included\n",
    "\n",
    "numeric_imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_imputer.fit(numeric_features_train)\n",
    "\n",
    "# For the validation data, ensure you're using the same features\n",
    "if 'price' in numeric_features_train.columns:\n",
    "    numeric_features_train = numeric_features_train.drop(columns=['price'], errors='ignore')\n",
    "numeric_features_val = validation_data[numeric_features_train.columns]  # This should no longer throw an error\n",
    "\n",
    "# Now apply the imputer to the validation data\n",
    "numeric_data_imputed_val = numeric_imputer.transform(numeric_features_val)\n",
    "\n",
    "\n",
    "# Assuming PCA is already fitted on the training data\n",
    "validation_principal_components = pca.transform(numeric_data_imputed_val)\n",
    "\n",
    "# Predict on the validation set using the trained Voting Regressor\n",
    "validation_predictions = voting_regressor.predict(validation_principal_components)\n",
    "print(\"Validation Predictions:\")\n",
    "print(validation_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions as a new column in the validation_data DataFrame\n",
    "validation_data['price'] = validation_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_data.columns)\n",
    "# Define the path where you want to save the file\n",
    "file_path = '/Users/elenalickel/Desktop/Machine Learning /Airbnb_NYC_2019_price_predictions.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "validation_data.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
